\documentclass{article}

% -----------------------------------------------------------------------------
% Includes
% -----------------------------------------------------------------------------

\usepackage{fullpage} % use more of the page for articles
\usepackage[utf8]{inputenc} % Use utf-8 encoding for foreign characters
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb} % More symbols
\usepackage{amsthm} % Theorems
\usepackage{mdwlist} % compact lists and \note environment
\usepackage{url} % URLs
\usepackage{hyperref}

\usepackage{parskip} % add space b/w pars instead of indenting 
\makeatletter % fix ams theorem spacing to work with parskip
\def\thm@space@setup{
  \thm@preskip=\parskip \thm@postskip=\parskip
}
\makeatother

\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

% -----------------------------------------------------------------------------
% Top Matter
% -----------------------------------------------------------------------------

\title{Implementing Bayes Classifiers}
\author{Gregory Bint \\
  Evan Huus \\
  Simon Pratt
  \footnote{Simon can be reached at
    \href{mailto:spratt@scs.carleton.ca}{spratt@scs.carleton.ca} for questions.} \\
  {\small Carleton University}}
\date{\today}

\bibliographystyle{plain}

\begin{document}

\maketitle

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Background}
\label{sec:Background}

Recall that if we have a sufficiently large collection of sample data
drawn from an inherently random set of variables then the
\emph{Central Limit Theorem} \cite{wiki_clt} states that this data
will approximate a \emph{Gaussian} or \emph{Normal} distribution
\cite{wiki_normal_dist}.
\begin{equation}
\label{e_sv_gauss}
\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}
\end{equation}

Here $\mu$ is the mean and $\sigma^2$ is the variance. This follows
from the idea that we \emph{expect} many of the samples to be close to
the expected value, or mean, of the underlying random variable.

When working with data that has multiple attributes or
\emph{dimensions}, such as the iris data, there is no such thing as
\emph{the} expected value, or \emph{the} sample value. Instead, we use
vectors and matrices to record each dimension of our data.

More specifically, if our data has $d$ dimensions (the iris data has
$4$), then:
\[ 
x \text{ becomes } \vec{x} = 
\begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_d \end{bmatrix}
\text { and } 
\mu \text{ becomes } M = 
\begin{bmatrix} \mu_1 \\ \mu_2 \\ \vdots \\ \mu_d \end{bmatrix}
\]

and $\sigma^2$ becomes the $d \times d$ matrix $\Sigma$, whose
contents depend on the type of classifier you are making, discussed
below.

If we modify Equation \ref{e_sv_gauss}, above, to use these
multi-dimensional variables, we get the Gaussian multivariate
formula\cite{wiki_mv_gauss}, which looks as follows:
\begin{equation}
\label{e_mv_gauss}
\dfrac{1}{\sqrt{(2\pi)^d |\Sigma|}} e^{-\dfrac{(\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)}{2}}
\end{equation}

We can then simplify Equation \ref{e_mv_gauss} to the following:
\begin{equation}
\label{e_mv_guass2}
\dfrac{1}{\sqrt{(2\pi)^d |\Sigma| e^{D'(\vec{x},M,\Sigma)}}}
\end{equation}

Where $D'$ is the square of the \emph{Mahalanobis} distance function
\cite{wiki_mahalanobis}.  The square of the \emph{Mahalanobis} distance
function is defined as:
\begin{displaymath}
  D'(\vec{x},M,\Sigma) = (\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)
\end{displaymath}

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Na\"ive Bayes Classifier}
\label{sec:nbayes}

The first thing to note is that the following formulas only work to
classify between two different classes.  If your training data
contains more than two classes, your first step is to choose two
classes of interest and disregard all other data.

The steps we follow to build the classifier are as follows:

\begin{enumerate}

\item Partition the training data into two sets, those belonging to class $A$
  and those belonging to class $B$.

\item Calculate the Normal Distribution for each set.

\item Form the \emph{classification equation}.

\end{enumerate}

\subsection{Partition the data}

This step is easy, simply look over the training data and place each
instance into the appropriate set according to its class.  Recall that
we know the class of each sample of our training data.

\subsection{Calculate the Normal Distribution Equations}
\label{ss:norm_dist}

Calculating the normal distribution of the training data is just a
matter of calculating $M$ and $\Sigma$.  In our case, we are
calculating the Sample Mean and the Sample Covariance
\cite{wiki_covariance} since we are working with sample data.

Sample mean is nothing more than a simple average.  For each dimension
$i$ of the sample data, calculate the sum total of the values over all
instances of the class partition, and then divide by the number of
instances to get $\mu_i$.

In Na\"ive Bayes, our covariance matrix is a diagonal matrix of the
following form:

\begin{displaymath}
\begin{bmatrix} \sigma^2_1 & 0 & \cdots & 0 \\
				 0 & \sigma^2_2 & \cdots & 0 \\
				 \vdots & \vdots & \ddots & \vdots \\
				 0 & 0 & \cdots & \sigma^2_d
\end{bmatrix}
\end{displaymath}

Each $\sigma_i$ represents the variance of the particular dimension of
the sample data.  Variance is calculated with the following:

\begin{displaymath}
 \sigma^2_i = E [{(x_i - \mu_i)}^2]
\end{displaymath}

Calculating this is just a matter of looping over every sample, and
for each one, we calculate the value $x_i - \mu_i$ (recall that we
calculated all the $\mu$'s earlier), add that value to a running total
(i.e. the summation step), and when finished, divide by the number of
samples.

Now that we have $M$ and $\Sigma$ for \emph{each} class, we have done
all the hard work of finding the Normal Distribution for this class.
We now use these two variables to create the classification equation.

\subsection{Build the Classification Equation}

Let $M_A$ and $\Sigma_A$ be the mean vector and covariance matrix for
class $A$, and let $M_B$ and $\Sigma_B$ be the mean vector and
covariance matrix for class $B$.

In simple terms, classification is performed by plugging an
instance of input data, $\vec{x}$, from our testing set, and
checking whether class $A$ or class $B$ is more likely (remember: this is all
just probabilities). This is equivalent to deciding if the output value of the Normal
Distribution equation is higher for class $A$'s equation, or class $B$'s.

\begin{equation}
\label{e_ce}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}} 
  \; \gtrless^{\omega_A}_{\omega_B} \; 
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}}
\end{equation}

This formula gives the probability that a given data point is in the
distribution.  Given the covariance matrix and mean vector for both
class A and class B, after many steps of simplification (which can be
seen in Appendix \ref{app:derivation}), we get the following
inequality:

\begin{equation}
\label{e_ce2}
ln|\Sigma_B| - ln|\Sigma_A| + D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
\; \gtrless^{\omega_A}_{\omega_B} \; 0 
\end{equation}

For performing the matrix calculations necessary to create this expression, we
recommend the Jama library, available at
\href{http://math.nist.gov/javanumerics/jama/}{http://math.nist.gov/javanumerics/jama/},
if you are programming in Java, or the NumPy library,
available at \href{http://numpy.scipy.org/}{http://numpy.scipy.org/}, if you are
programming in Python.

\subsection{Classify!}

Equation \ref{e_ce2} is the final product we are looking for. With this equation
in hand, we can classify any instance of our data, $\vec{x}$, by simply plugging
it in.  The output from the equation will be a single, scalar number. If that
number is greater than $0$, then $\vec{x}$ \emph{most likely} belongs to class
$A$, otherwise $\vec{x}$ belongs to class $B$.

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Optimal Bayes Classifier}
\label{sec:obayes}

The only difference between Na\"ive Bayes and Optimal Bayes classifiers
is the covariance matrix used.  In a Na\"ive Bayes classifier, the covariance
matrix is just a diagonal matrix (Section \ref{ss:norm_dist}), but in an
Optimal Bayes classifier, we use a full matrix instead, which has the following
structure:

\begin{displaymath}
\begin{bmatrix} \sigma_1\sigma_1 & \sigma_1\sigma_2 & \cdots & \sigma_1\sigma_d \\
				 \sigma_2\sigma_1 & \sigma_2\sigma_2 & \cdots & \sigma_2\sigma_d \\
				 \vdots & \vdots & \ddots & \vdots \\
				 \sigma_d\sigma_1 & \sigma_d\sigma_2 & \cdots & \sigma_d\sigma_d
\end{bmatrix}
\end{displaymath}

Where each entry is calculated by:

\begin{displaymath}
 \sigma_i\sigma_j = E [{(x_i - \mu_i)(x_j - \mu_j)}]
\end{displaymath}

All of the other steps and procedures for training and testing are the
same, the only difference is the complexity of the covariance matrix.


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Programming a Classifier}
\label{sec:tutorial}

Now that we understand the theory, we can implement a working
classifier.  We are given $n$ data points, each consisting of $k$
numerical observations and a classification.

This data is often stored as a file wherein each line contains
observations, an indication of the classification, and possibly
extraneous data.  This must be parsed and stored in some convenient
data structure of your own design.

\subsection{Partition the Data into Training and Testing Sets}

We begin by partitioning the dataset into a \emph{training set} and a
\emph{testing set}.  The purpose of the \emph{training set} is to
build the covariance matrix and mean vector for each class.  The
purpose of the \emph{testing set} is to provide independent data with
which to test the accuracy of the classifier.

The simplest way to partition the dataset is to choose the first
$\ceil{n/2}$ tuples for the \emph{training set} and the remaining
$\floor{n/2}$ tuples for the \emph{testing set}.  We will discuss
other partitioning schemes in Section \ref{ss_testing}.

\subsection{Partition the Training Set by Class}

Next, we further partition the \emph{training set} by class.  If there
are two classes, $A$ and $B$, let the set of data points belonging to
class $A$ be $T_A$ and similarly let the set of data points belonging
to $B$ be $T_B$.

It is important to note that if either $T_A$ or $T_B$ is empty,
classification will almost certainly be hopelessly inaccurate.  It is
important to have some strategy to handle this case, and a reasonable
strategy would be to throw an error.

\subsection{Calculate the Mean Vector}

A mean vector is a $k$-dimensional vector $M$ (recall that $k$ is the number
of numerical observations in each data point), in which the
$i$\textsuperscript{th} element is the mean of all $i$\textsuperscript{th}
numerical observations. It is constructed by first building a sum vector whose
$i$\textsuperscript{th} element is the sum of all $i$\textsuperscript{th}
numerical observations, and then dividing each element by the number of data
points.

We will need to do this twice, once to build the mean vector $M_A$ for training
data $T_A$ and again, separately, to build $M_B$ for the training set $T_B$.

\subsection{Calculate the Covariance Matrix}

Whether we are building a Na\"ive Bayes or Optimal Bayes classifier, we must
calculate the variances for each observation, similar to how we calculated
the mean for each observation in the previous step.

We construct the covariance matrix by instantiating a $k \times k$ matrix,
$\Sigma$, with zeroes, and iterating over the data points.  For each data point
(itself a $k$-dimensional vector), we subtract our mean vector $M$. We take the
outer product of the result (itself times its transpose) which results in a
new $k \times k$ matrix. This new matrix gets added to $\Sigma$. Once we've
iterated over every data point, we divide each element of $\Sigma$ by
\emph{one less than} the number of data points.

As with the mean vector, we must repeat this procedure twice, as we must
independently build $\Sigma_A$ (with respect to $M_A$ and
$T_A$) and $\Sigma_B$ (with respect to $M_B$ and $T_B$).

The formula for $\Sigma_A$ is as follows:

\[ \Sigma_A = \frac{1}{n-1} \sum_{i=1}^{n} (x_i - M_A)(x_i - M_A)^{T}
\]

If we are building a Na\"ive Bayes classifier, we can take the ``full''
covariance matrices we just created for the Optimal Bayes classifier and simply
set every element to $0$, except for those found along the main diagonal (that
is, the diagonal starting at the top left and ending at the bottom right).

\subsection{Classify!}

We now have everything we need in order to use Equation \ref{e_ce2}, repeated
here for reference:

\begin{equation*}
ln|\Sigma_B| - ln|\Sigma_A| + D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
\; \gtrless^{\omega_A}_{\omega_B} \; 0 
\end{equation*}

Recall that $\vec{x}$ is a vector of numerical observations from our
\emph{testing set} that we would like to classify. We calculate the
classification value, $c$, as follows:
\[\begin{split}
  c 
  &= ln|\Sigma_B| - ln|\Sigma_A| + D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A) \\
  &= ln|\Sigma_B| - ln|\Sigma_A| + (\vec{x}-M_B)^T \Sigma_B^{-1} (\vec{x}-M_B) -(\vec{x}-M_A)^T \Sigma_A^{-1} (\vec{x}-M_A)
\end{split} \]

If $c$ is greater than zero, the observations most likely belong
to class $A$, otherwise class $B$, and our program should report as much.

\begin{note}
Be careful when calculating the inverse of the covariance matrix,
because it may be singular, i.e. have no inverse.  In this case, your
library may provide a \emph{pseudo-inverse}\cite{wiki_pinv} which is
a safe approximation of the true inverse. You will also need to calculate the
\emph{pseudo-determinant}\cite{wiki_pdet}, since the determinant of a singular
matrix is $0$, and the logarithm of $0$ is undefined.
\end{note}

\subsection{Testing}
\label{ss_testing}

In the first step, we partitioned the sample into a \emph{Training
  Set} and a \emph{Testing Set}.  To test, we simply run the
  classifier against all the observations in the \emph{Testing Set}
  and keep a score of how many are correctly classified.

To more accurately test the classifier, you could randomize which
observations are used to train the classifier.

You could also use $K$-fold cross-validation in which the observations
are partitioned into $K$ groups and $K-1$ of the groups are used to
train and the remaining group is used to test.  Using the same
partitioning, a new classifier is trained on $K-1$ of the groups and a
different group is used to test.  This is repeated $K$ times such that
every group is used to test once.

Finally, both randomization and $K$-fold cross-validation can be used
together and the results averaged.

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\appendix
\section{Derivation}
\label{app:derivation}

The full derivation from Equation \ref{e_ce} to Equation \ref{e_ce2} is
as follows:

\begin{align*}
  %
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}} 
  &\gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}} \\
  %
  \left(\dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}}\right)^2
  &\gtrless^{\omega_A}_{\omega_B}
  \left(\dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}}\right)^2 \\
  %
  \dfrac{1}{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}} \\
  %
  \dfrac{1}{|\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{|\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}} \\
  %
  \ln\left(\dfrac{1}{|\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}\right)
  &\gtrless^{\omega_A}_{\omega_B}
  \ln\left(\dfrac{1}{|\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}\right) \\
  %
  -\ln|\Sigma_A| - \ln(e^{D'(\vec{x},M_A,\Sigma_A)})
  &\gtrless^{\omega_A}_{\omega_B}
  -\ln|\Sigma_B| - \ln(e^{D'(\vec{x},M_B,\Sigma_B)}) \\
  %
  \ln|\Sigma_B| - \ln|\Sigma_A| +
  D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0
  %
\end{align*}

\bibliography{references}

\end{document}
