\documentclass{article}

% -----------------------------------------------------------------------------
% Includes
% -----------------------------------------------------------------------------

\usepackage{fullpage} % use more of the page for articles
\usepackage[utf8]{inputenc} % Use utf-8 encoding for foreign characters
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb} % More symbols
\usepackage{mdwlist} % compact lists
\usepackage{hyperref}
\usepackage{amsthm} % Theorems
\usepackage{url} % URLs
\usepackage{hyperref}
\usepackage{color}

% -----------------------------------------------------------------------------
% Configuration
% -----------------------------------------------------------------------------


% -----------------------------------------------------------------------------
% Custom Stuff
% -----------------------------------------------------------------------------

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{openproblem}{Open Problem}
\newtheorem{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{property}{Property}

\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

% column vector stuff
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{bmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{bmatrix}
        \fi
}


% -----------------------------------------------------------------------------
% Top Matter
% -----------------------------------------------------------------------------

\title{Implementing Bayes Classifiers}
\author{Gregory Bint \\
  Simon Pratt
  \footnote{Simon can be reached at
    \href{mailto:spratt@scs.carleton.ca}{spratt@scs.carleton.ca} for questions.} \\
  {\small Carleton University}}
\date{\today}

\bibliographystyle{plain}

\begin{document}

\maketitle

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Background}
\label{sec:Background}

Recall from STAT 2605 that if we have a sufficiently large collection
of sample data drawn from an inherently random set of variables, that
the \emph{Central Limit Theorem} \cite{wiki_clt} states that this data
will approximate a \emph{Gaussian} or \emph{Normal} distribution
\cite{wiki_normal_dist}.

\begin{displaymath}
\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}
\end{displaymath}

Here $\mu$ is the mean and $\sigma^2$ is the variance. This follows
from the idea that we \emph{expect} many of the samples to be close to
the expected value, or mean, of the underlying random variable.

When working with data that has multiple attributes or
\emph{dimensions}, such as the iris data, there is no such thing as
\emph{the} expected value, or \emph{the} sample value. Instead, we use
vectors and matrices to record each dimension of our data.

More specifically, if our data has $d$ dimensions (the iris data has
$4$), then:

\[ 
x \text{ becomes } \vec{x} = \colvec{4}{x_1}{x_2}{\vdots}{x_d}
\text { and } 
\mu \text{ becomes } M = \colvec{4}{\mu_1}{\mu_2}{\vdots}{\mu_d}
\]

and $\sigma^2$ becomes the $d \times d$ matrix $\Sigma$, whose
contents depend on the type of classifier you are making, discussed
below.

The Gaussian multivariate formula is now the following\cite{wiki_mv_gauss}:

\begin{displaymath}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma|}} e^{-\dfrac{(\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)}{2}}
  = \dfrac{1}{\sqrt{(2\pi)^d |\Sigma| e^{(\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)}}}
  = \dfrac{1}{\sqrt{(2\pi)^d |\Sigma| e^{D'(\vec{x},M,\Sigma)}}}
\end{displaymath}

Where $D'$ is the square of the \emph{Mahalanobis} distance function
\cite{wiki_mahalanobis}.  The square of the \emph{Mahalanobis} distance
function is defined as:

\begin{displaymath}
  D'(\vec{x},M,\Sigma) = (\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)
\end{displaymath}

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Na\"ive Bayes Classifier}
\label{sec:nbayes}

The first thing to note is that the following formulas only work to
classify between two different classes.  If your training data
contains more than two classes, your first step is to choose two
classes of interest and disregard all other data.

The steps we follow to build the classifier are as follows:

\begin{enumerate}

\item Partition the data into two sets, those belonging to class $A$
  and those belonging to class $B$.

\item Calculate the Normal Distribution for each set.

\item Form the \emph{classification equation}.

\end{enumerate}

\subsection{Partition}

This step is easy, simply look over the training data and place each
instance into the appropriate set according to its class.  Recall that
we know the class of each sample of our training data.

\subsection{Calculate Normal Distribution Equations}
\label{ss:norm_dist}

Calculating the normal distribution of the training data is just a
matter of calculating $M$ and $\Sigma$.  In our case, we are
calculating the Sample Mean and the Sample Covariance
\cite{wiki_covariance} since we are working with sample data.

Sample mean is nothing more than a simple average.  For each dimension
$i$ of the sample data, calculate the sum total of the values over all
instances of the class partition, and then divide by the number of
instances to get $\mu_i$.

In Na\"ive Bayes, our covariance matrix is a diagonal matrix of the
following form:

\begin{displaymath}
\begin{bmatrix} \sigma^2_1 & 0 & \cdots & 0 \\
				 0 & \sigma^2_2 & \cdots & 0 \\
				 \vdots & \vdots & \ddots & \vdots \\
				 0 & 0 & \cdots & \sigma^2_d
\end{bmatrix}
\end{displaymath}

Each $\sigma_i$ represents the variance of the particular dimension of
the sample data.  Variance is calculated with the following:

\begin{displaymath}
 \sigma^2_i = E [{(x_i - \mu_i)}^2]
\end{displaymath}

Calculating this is just a matter of looping over every sample, and
for each one, calculate the value $x_i - \mu_i$ (recall that we
calcuated all the $\mu$'s earlier), add that value to a running total
(i.e. the summation step), and when finished, divide by the number of
samples.

Now that we have $M$ and $\Sigma$ for \emph{each} class, we have done
all the hard work of finding the Normal Distribution for this class.
We now use these two variables to create the classification equation.

\subsection{Build Classifier Equation}

Let $M_A$ and $\Sigma_A$ be the mean vector and covariance matrix for
class $A$, and let $M_B$ and $\Sigma_B$ be the mean vector and
covariance matrix for class $B$.

In simple terms, classification is performed by plugging in the
desired instance of input data $\vec{x}$ from our testing set, and
seeing whether class $A$ or class $B$ is more likely at that
point. This is equivalent to deciding if the output value of the
Normal Distribution equation is higher for class $A$'s equation, or
class $B$'s.

\begin{displaymath}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}} 
  \gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}}
\end{displaymath}

This formula gives the probability that a given data point is in the
distribution.  Given the covariance matrix and mean vector for both
class A and class B, we have the following inequality that determines
if a sample vector $\vec{x}$ is in class A or B:

\begin{align*}
  %
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}} 
  &\gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}} \\
  %
  \frac{\sqrt{(2\pi)^d |\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}}{\sqrt{(2\pi)^d |\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}}
  &\gtrless^{\omega_A}_{\omega_B}
  1 \\
  %
  \left(
  \frac{\sqrt{(2\pi)^d |\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}}{\sqrt{(2\pi)^d |\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}}
  \right)^2
  &\gtrless^{\omega_A}_{\omega_B}
  1^2 \\
  %
  \frac{(2\pi)^d |\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{(2\pi)^d |\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  1 \\
  %
  \frac{|\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{|\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  \frac{(2\pi)^d}{(2\pi)^d} \\
  %
  \frac{|\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{|\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  1 \\
  %
  ln \left(
  \frac{|\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{|\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  \right)
  &\gtrless^{\omega_A}_{\omega_B}
  ln(1) \\
  %
  ln(|\Sigma_B|e^{D'(\vec{x},M_B,\Sigma_B)}) -
  ln(|\Sigma_A|e^{D'(\vec{x},M_A,\Sigma_A)})
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| + ln(e^{D'(\vec{x},M_B,\Sigma_B)}) -
  ln|\Sigma_A| - ln(e^{D'(\vec{x},M_A,\Sigma_A)})
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| + D'(\vec{x},M_B,\Sigma_B)ln(e) -
  ln|\Sigma_A| - D'(\vec{x},M_A,\Sigma_A)ln(e)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| + D'(\vec{x},M_B,\Sigma_B) -
  ln|\Sigma_A| - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| - ln|\Sigma_A| +
  D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
\end{align*}

I recommend the Jama library, available at
\href{http://math.nist.gov/javanumerics/jama/}{http://math.nist.gov/javanumerics/jama/}
for performing the matrix calculations.  Or the NumPy library,
available at \href{http://numpy.scipy.org/}{http://numpy.scipy.org/}.

\subsection{Classify!}

Now that we have our formula, to classify any one instance of
$\vec{x}$, we just plug it into our formula.  The result will be a
scalar number, which indicates class $A$ if less than $0$ and class
$B$ otherwise.


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Optimal Bayes Classifier}
\label{sec:obayes}

The only difference between Na\"ive Bayes and Optimal Bayes classifiers
is the covariance matrix used.  In the Na\"ive classifier, the matrix
was just a diagonal matrix (Section \ref{ss:norm_dist}), but in the optimal one,
a full matrix is used, which has the following structure.

\begin{displaymath}
\begin{bmatrix} \sigma_1\sigma_1 & \sigma_1\sigma_2 & \cdots & \sigma_1\sigma_d \\
				 \sigma_2\sigma_1 & \sigma_2\sigma_2 & \cdots & \sigma_2\sigma_d \\
				 \vdots & \vdots & \ddots & \vdots \\
				 \sigma_d\sigma_1 & \sigma_d\sigma_2 & \cdots & \sigma_d\sigma_d
\end{bmatrix}
\end{displaymath}

Each of those entries is calculated by:

\begin{displaymath}
 \sigma_i\sigma_j = E [{(x_i - \mu_i)(x_j - \mu_j)}]
\end{displaymath}

All of the other steps and procedures for training and testing are the
same, the only difference is the complexity of the covariance matrix.


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Hints}
\label{sec:hints}

Be careful when calculating covariance matrices with respect to your
input data. If you have no instances of data for a particular class,
then your covariance matrix may be singular and therefore not
invertible!  This can be quickly determined by checking that the
determinant is not zero.  You may also want to look into the Pseudo-Inverse of a
matrix as detailed in \cite{wiki_pinv}.

\bibliography{references}

\end{document}
