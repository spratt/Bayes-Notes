\documentclass{article}

% -----------------------------------------------------------------------------
% Includes
% -----------------------------------------------------------------------------

\usepackage{fullpage} % use more of the page for articles
\usepackage[utf8]{inputenc} % Use utf-8 encoding for foreign characters
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb} % More symbols
\usepackage{mdwlist} % compact lists
\usepackage{hyperref}
\usepackage{amsthm} % Theorems
\usepackage{url} % URLs
\usepackage{hyperref}
\usepackage{color}

% -----------------------------------------------------------------------------
% Configuration
% -----------------------------------------------------------------------------


% -----------------------------------------------------------------------------
% Custom Stuff
% -----------------------------------------------------------------------------

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{openproblem}{Open Problem}
\newtheorem{problem}{Problem}
\newtheorem{definition}{Definition}
\newtheorem{property}{Property}

\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\ceil}[1]{\left\lceil#1\right\rceil}

% column vector stuff
\newcount\colveccount
\newcommand*\colvec[1]{
        \global\colveccount#1
        \begin{bmatrix}
        \colvecnext
}
\def\colvecnext#1{
        #1
        \global\advance\colveccount-1
        \ifnum\colveccount>0
                \\
                \expandafter\colvecnext
        \else
                \end{bmatrix}
        \fi
}


% -----------------------------------------------------------------------------
% Top Matter
% -----------------------------------------------------------------------------

\title{Implementing Bayes Classifiers}
\author{Gregory Bint \\
  Simon Pratt
  \footnote{Simon can be reached at
    \href{mailto:spratt@scs.carleton.ca}{spratt@scs.carleton.ca} for questions.} \\
  {\small Carleton University}}
\date{\today}

\bibliographystyle{plain}

\begin{document}

\maketitle

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Background}
\label{sec:Background}

Recall that if we have a sufficiently large collection of sample data
drawn from an inherently random set of variables, that the
\emph{Central Limit Theorem} \cite{wiki_clt} states that this data
will approximate a \emph{Gaussian} or \emph{Normal} distribution
\cite{wiki_normal_dist}.

\begin{displaymath}
\dfrac{1}{\sqrt{2\pi}\sigma}e^{-\dfrac{(x-\mu)^2}{2\sigma^2}}
\end{displaymath}

Here $\mu$ is the mean and $\sigma^2$ is the variance. This follows
from the idea that we \emph{expect} many of the samples to be close to
the expected value, or mean, of the underlying random variable.

When working with data that has multiple attributes or
\emph{dimensions}, such as the iris data, there is no such thing as
\emph{the} expected value, or \emph{the} sample value. Instead, we use
vectors and matrices to record each dimension of our data.

More specifically, if our data has $d$ dimensions (the iris data has
$4$), then:

\[ 
x \text{ becomes } \vec{x} = \colvec{4}{x_1}{x_2}{\vdots}{x_d}
\text { and } 
\mu \text{ becomes } M = \colvec{4}{\mu_1}{\mu_2}{\vdots}{\mu_d}
\]

and $\sigma^2$ becomes the $d \times d$ matrix $\Sigma$, whose
contents depend on the type of classifier you are making, discussed
below.

The Gaussian multivariate formula is now the following\cite{wiki_mv_gauss}:

\begin{displaymath}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma|}} e^{-\dfrac{(\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)}{2}}
  = \dfrac{1}{\sqrt{(2\pi)^d |\Sigma| e^{(\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)}}}
  = \dfrac{1}{\sqrt{(2\pi)^d |\Sigma| e^{D'(\vec{x},M,\Sigma)}}}
\end{displaymath}

Where $D'$ is the square of the \emph{Mahalanobis} distance function
\cite{wiki_mahalanobis}.  The square of the \emph{Mahalanobis} distance
function is defined as:

\begin{displaymath}
  D'(\vec{x},M,\Sigma) = (\vec{x}-M)^T \Sigma^{-1} (\vec{x}-M)
\end{displaymath}

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Na\"ive Bayes Classifier}
\label{sec:nbayes}

The first thing to note is that the following formulas only work to
classify between two different classes.  If your training data
contains more than two classes, your first step is to choose two
classes of interest and disregard all other data.

The steps we follow to build the classifier are as follows:

\begin{enumerate}

\item Partition the data into two sets, those belonging to class $A$
  and those belonging to class $B$.

\item Calculate the Normal Distribution for each set.

\item Form the \emph{classification equation}.

\end{enumerate}

\subsection{Partition the data}

This step is easy, simply look over the training data and place each
instance into the appropriate set according to its class.  Recall that
we know the class of each sample of our training data.

\subsection{Calculate the Normal Distribution Equations}
\label{ss:norm_dist}

Calculating the normal distribution of the training data is just a
matter of calculating $M$ and $\Sigma$.  In our case, we are
calculating the Sample Mean and the Sample Covariance
\cite{wiki_covariance} since we are working with sample data.

Sample mean is nothing more than a simple average.  For each dimension
$i$ of the sample data, calculate the sum total of the values over all
instances of the class partition, and then divide by the number of
instances to get $\mu_i$.

In Na\"ive Bayes, our covariance matrix is a diagonal matrix of the
following form:

\begin{displaymath}
\begin{bmatrix} \sigma^2_1 & 0 & \cdots & 0 \\
				 0 & \sigma^2_2 & \cdots & 0 \\
				 \vdots & \vdots & \ddots & \vdots \\
				 0 & 0 & \cdots & \sigma^2_d
\end{bmatrix}
\end{displaymath}

Each $\sigma_i$ represents the variance of the particular dimension of
the sample data.  Variance is calculated with the following:

\begin{displaymath}
 \sigma^2_i = E [{(x_i - \mu_i)}^2]
\end{displaymath}

Calculating this is just a matter of looping over every sample, and
for each one, calculate the value $x_i - \mu_i$ (recall that we
calculated all the $\mu$'s earlier), add that value to a running total
(i.e. the summation step), and when finished, divide by the number of
samples.

Now that we have $M$ and $\Sigma$ for \emph{each} class, we have done
all the hard work of finding the Normal Distribution for this class.
We now use these two variables to create the classification equation.

\subsection{Build the Classifier Equation}

Let $M_A$ and $\Sigma_A$ be the mean vector and covariance matrix for
class $A$, and let $M_B$ and $\Sigma_B$ be the mean vector and
covariance matrix for class $B$.

In simple terms, classification is performed by plugging in the
desired instance of input data $\vec{x}$ from our testing set, and
seeing whether class $A$ or class $B$ is more likely (remember: this is all just
probabilities). This is equivalent to deciding if the output value of the Normal
Distribution equation is higher for class $A$'s equation, or class $B$'s.

\begin{displaymath}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}} 
  \gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}}
\end{displaymath}

This formula gives the probability that a given data point is in the
distribution.  Given the covariance matrix and mean vector for both
class A and class B, after many steps of simplification (which can be
seen in Appendix \ref{app:derivation}), we get the following
inequality:

\begin{align*}
  %
  ln|\Sigma_B| - ln|\Sigma_A| +
  D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
\end{align*}

We recommend the Jama library, available at
\href{http://math.nist.gov/javanumerics/jama/}{http://math.nist.gov/javanumerics/jama/}
for performing the matrix calculations.  Or the NumPy library,
available at \href{http://numpy.scipy.org/}{http://numpy.scipy.org/}.

\subsection{Classify!}

Now that we have our formula, to classify any one instance of
$\vec{x}$, we just plug it into our formula.  The result will be a
scalar number, which indicates class $A$ if it is less than $0$ and class
$B$ otherwise.


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------

\section{Optimal Bayes Classifier}
\label{sec:obayes}

The only difference between Na\"ive Bayes and Optimal Bayes classifiers
is the covariance matrix used.  In the Na\"ive classifier, the matrix
was just a diagonal matrix (Section \ref{ss:norm_dist}), but in the optimal one,
a full matrix is used, which has the following structure.

\begin{displaymath}
\begin{bmatrix} \sigma_1\sigma_1 & \sigma_1\sigma_2 & \cdots & \sigma_1\sigma_d \\
				 \sigma_2\sigma_1 & \sigma_2\sigma_2 & \cdots & \sigma_2\sigma_d \\
				 \vdots & \vdots & \ddots & \vdots \\
				 \sigma_d\sigma_1 & \sigma_d\sigma_2 & \cdots & \sigma_d\sigma_d
\end{bmatrix}
\end{displaymath}

Each of those entries is calculated by:

\begin{displaymath}
 \sigma_i\sigma_j = E [{(x_i - \mu_i)(x_j - \mu_j)}]
\end{displaymath}

All of the other steps and procedures for training and testing are the
same, the only difference is the complexity of the covariance matrix.


% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\section{Building A Classifier}
\label{sec:tutorial}

With this understanding of the theory, we can build a working
classifier.  We are given $n$ data points, each consisting of $k$
numerical observations and a classification.

This data is often stored as a file wherein each line contains
observations, an indication of the classification, and possibly
extraneous data.  This must be parsed and stored in some convenient
structure.

\subsection{Partition the Data into Training and Testing Sets}

We begin by partition the dataset into a \emph{training set} and a
\emph{testing set}.  The purpose of the \emph{training set} is to
build the covariance matrix and mean vector for each class.  The
purpose of the \emph{testing set} is to provide independent data with
which to test the accuracy of the classifier.

The simplest way to partition the dataset is to choose the first
$\ceil{n/2}$ tuples for the \emph{training set} and the remaining
$\floor{n/2}$ tuples for the \emph{testing set}.  We will discuss
other partitioning schemes at the end of this section.

\subsection{Partition the Training Set by Class}

Next, we further partition the \emph{training set} by class.  If there
are two classes $A$ and $B$, let the set of data points belonging to
class $A$ be $T_A$ and similarly let the set of data points belonging
to $B$ be $T_B$.

It is important to note that if either $T_A$ or $T_B$ is empty,
classification will almost certainly be hopelessly inaccurate.  It is
important to have some strategy to handle this case, and a reasonable
strategy would be to throw an error.

\subsection{Calculate the Mean Vectors}

The mean vector is a $k$-vector $M$ (where $k$ is the number of
numerical observations in each data point), in which the $i$th element
is the mean of all $i$th numerical observations.  We construct this by
first building a sum vector whose $i$th element is the sum of all
$i$th numerical observations, then divide each element by the number
of data points.

We do this to build a mean vector $M_A$ for $T_A$ and $M_B$ for $T_B$
separately.

\subsection{Calculate the Variance Vector}

Whether we are building a na\"ive or optimal classifier, we must
calculate the variances for each observation just as we calculated the
mean for each observation in the previous step.

We construct the variance vector by instantiating a $k$-vector $V$ and
iterating over the data points.  For the $i$th element of each data
point, we subtract from its value the $i$th element in our mean
vector.  We square this value and add the result to the $i$th element
of $V$.  Once we've iterated over every data point, we divide each
element of $V$ by the number of data points, and take the square root.

We do this to build a variance vector $V_A$ for $T_A$ and $V_B$ for
$T_B$ separately.

\subsection{Calculate the Covariance Matrix}

If we are building the optimal classifier and our variance vector $V$
is a column vector, our covariance matrix is simply $V^TV$.

We have $\Sigma_A = V_A^TV_A$ and $\Sigma_B = V_B^TV_B$.

If we are building the na\"ive classifier, we take the covariance
matrices from the optimal classifier and simply set every element to
$0$ except along the diagonal starting at the top left and ending at
the bottom right.

\subsection{Classify!}

Now we use the formula we derived earlier:

\begin{align*}
  %
  ln|\Sigma_B| - ln|\Sigma_A| +
  D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
\end{align*}

Where $\vec{x}$ is a vector of numerical observations we would like to
classify.

We calculate the classification value $c$:

\begin{align*}
  %
  c 
  &=
  ln|\Sigma_B| - ln|\Sigma_A| +
  D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A) \\
  &=
  ln|\Sigma_B| - ln|\Sigma_A| +
  (\vec{x}-M_B)^T \Sigma_B^{-1} (\vec{x}-M_B) - 
  (\vec{x}-M_A)^T \Sigma_A^{-1} (\vec{x}-M_A)
  %
\end{align*}

If this $c$ is greater than zero, the observations most likely belong
to class $A$, otherwise class $B$.

Be careful when calculating the inverse of the covariance matrix,
because it may be singular, i.e. have no inverse.  In this case, your
library may provide a \emph{Pseudo-Inverse}\cite{wiki_pinv} which is
an approximation of the

\subsection{Testing}

In the first step, we partitioned the sample into a \emph{Training
  Set} and a \emph{Testing Set}.  To test, we simply run the
  classifier against all the observations in the \emph{Testing Set}
  and keep a score of how many are correctly classified.

To more accurately test the classifier, you could randomize which
observations are used to train the classifier.

You could also use $K$-fold cross-validation in which the observations
are partitioned into $K$ groups and $K-1$ of the groups are used to
train and the remaining group is used to test.  Using the same
partitioning, a new classifier is trained on $K-1$ of the groups and a
different group is used to test.  This is repeated $K$ times such that
every group is used to test once.

Finally, both randomization and $K$-fold cross-validation can be used
together and the results averaged.

% -----------------------------------------------------------------------------
% -----------------------------------------------------------------------------
\appendix
\section{Derivation}
\label{app:derivation}

The full derivation of the given formula follows:

\begin{align*}
  %
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_A| e^{D'(\vec{x},M_A,\Sigma_A)}}} 
  &\gtrless^{\omega_A}_{\omega_B}
  \dfrac{1}{\sqrt{(2\pi)^d |\Sigma_B| e^{D'(\vec{x},M_B,\Sigma_B)}}} \\
  %
  \frac{\sqrt{(2\pi)^d |\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}}{\sqrt{(2\pi)^d |\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}}
  &\gtrless^{\omega_A}_{\omega_B}
  1 \\
  %
  \left(
  \frac{\sqrt{(2\pi)^d |\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}}{\sqrt{(2\pi)^d |\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}}
  \right)^2
  &\gtrless^{\omega_A}_{\omega_B}
  1^2 \\
  %
  \frac{(2\pi)^d |\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{(2\pi)^d |\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  1 \\
  %
  \frac{|\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{|\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  \frac{(2\pi)^d}{(2\pi)^d} \\
  %
  \frac{|\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{|\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  &\gtrless^{\omega_A}_{\omega_B}
  1 \\
  %
  ln \left(
  \frac{|\Sigma_B|
      e^{D'(\vec{x},M_B,\Sigma_B)}}{|\Sigma_A|
      e^{D'(\vec{x},M_A,\Sigma_A)}}
  \right)
  &\gtrless^{\omega_A}_{\omega_B}
  ln(1) \\
  %
  ln(|\Sigma_B|e^{D'(\vec{x},M_B,\Sigma_B)}) -
  ln(|\Sigma_A|e^{D'(\vec{x},M_A,\Sigma_A)})
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| + ln(e^{D'(\vec{x},M_B,\Sigma_B)}) -
  ln|\Sigma_A| - ln(e^{D'(\vec{x},M_A,\Sigma_A)})
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| + D'(\vec{x},M_B,\Sigma_B)ln(e) -
  ln|\Sigma_A| - D'(\vec{x},M_A,\Sigma_A)ln(e)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| + D'(\vec{x},M_B,\Sigma_B) -
  ln|\Sigma_A| - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
  ln|\Sigma_B| - ln|\Sigma_A| +
  D'(\vec{x},M_B,\Sigma_B) - D'(\vec{x},M_A,\Sigma_A)
  &\gtrless^{\omega_A}_{\omega_B}
  0 \\
  %
\end{align*}

\bibliography{references}

\end{document}
